{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fc7931-543b-4822-bab4-be9b079f39c7",
   "metadata": {},
   "source": [
    "https://medium.com/@octaviopavon7/local-rag-with-local-llm-huggingface-chroma-5e0fc3b6133a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67161735-52cc-41ea-b3c2-b03220224828",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chroma.py\n",
    "from chromadb import Client, ClientAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc21d500-3b96-440b-8f78-ddbd66c8374b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chroma():\n",
    "    \"\"\"\n",
    "      Chroma class to instantiate a vector db in memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, default_database: str = \"default\", first_collection_name: str = \"test\", top_k: int = 1):\n",
    "        self.api: ClientAPI = Client()\n",
    "        self.collection_pointer = self.api.create_collection(first_collection_name)\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def new_collection(self ,name: str, **kwargs):\n",
    "        try:\n",
    "            self.api.create_collection(name, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def add_data_to(self, data):\n",
    "        try:\n",
    "            self.collection_pointer.add(\n",
    "                embeddings=data.get(\"embeddings\"),\n",
    "                documents=data.get(\"contents\"),\n",
    "                metadatas=data.get(\"metadatas\"),\n",
    "                ids=data.get(\"ids\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def switch_collection(self, new_pointer: str):\n",
    "        try:\n",
    "            self.collection_pointer = self.api.get_collection(new_pointer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def query(self, embedding: list[float], **kwargs):\n",
    "        try:\n",
    "            result = self.collection_pointer.query(query_embeddings=embedding, n_results=self.top_k, **kwargs)\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d30eed-b3c6-48a3-8afa-705eef0e72fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chunking.py\n",
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6f4175-2373-49f7-ba48-f7c50a2f65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file, ext: str, chunking:int=400,overlap:int=20, **kwargs):\n",
    "    if ext == \"pdf\":\n",
    "        return extract_pdf(file, chunking, overlap, **kwargs)\n",
    "    if ext == \"txt\":\n",
    "        return extract_text(file,chunking, overlap, **kwargs)\n",
    "    return extract_excel(file,chunking, overlap, **kwargs)\n",
    "\n",
    "def extension(file_name:str):\n",
    "    return file_name.split(\".\")[-1]\n",
    "\n",
    "def extract_pdf(file,chunking, overlap, **kwargs):\n",
    "    loader = UnstructuredPDFLoader(file)\n",
    "    return load(loader, chunking, overlap, **kwargs)\n",
    "\n",
    "def extract_text(file,chunking, overlap, **kwargs):\n",
    "    loader = TextLoader(file, 'utf-8')\n",
    "    return load(loader, chunking, overlap, **kwargs)\n",
    "\n",
    "def load(loader: TextLoader | UnstructuredPDFLoader, chunking, overlap, **kwargs):\n",
    "    return loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunking, chunk_overlap=overlap),**kwargs)\n",
    "\n",
    "\n",
    "def get_file_chunks(file_bytes: bytes, file_name: str, chunk_size: int = 200, chunk_overlap: int = 10):\n",
    "    try:\n",
    "        # Using temp-directory to store our read file for a little while\n",
    "        # This is because langchain loaders only acept file paths and not its raw bytes.\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, file_name)\n",
    "            \n",
    "            with open(temp_file_path, \"wb\") as temp_file:\n",
    "                temp_file.write(file_bytes)\n",
    "                \n",
    "            chunks = extract(temp_file_path, extension(file_name), chunk_size, chunk_overlap)\n",
    "\n",
    "            return chunks\n",
    "    except Exception as e:\n",
    "        print(\"ERROR TRYING TO GET CHUNKS FROM FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c86488-9717-4ff1-a544-a0ad737bbd47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding.py\n",
    "from openai import OpenAI   \n",
    "from os import environ as env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78efaf9f-b4dc-4dd9-b475-71e6baa82471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = \"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\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def embedding(input: str):\n",
    "    return client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=input,\n",
    "    ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee49e243-06f6-4141-9f68-cdf2e833c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "#from chroma import Chroma\n",
    "#from chunking import get_file_chunks\n",
    "from langchain.schema import Document\n",
    "#from embeddings import embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99aa838c-7197-4bd4-b121-48bf91720c95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loader():\n",
    "\n",
    "    def __init__(self, chroma_instance: Chroma):\n",
    "        self.chroma = chroma_instance\n",
    "        self.files_read = []\n",
    "    \n",
    "    def load_document(self, path: str):\n",
    "        chunks = self.chunk_document(path)\n",
    "        self.insert_data_to_chroma(chunks)\n",
    "        print(\"Data added successfully.\")\n",
    "\n",
    "    def chunk_document(self, path: str):\n",
    "        file = open(path, \"rb\")\n",
    "        bytes = file.read()\n",
    "        if bytes:\n",
    "            self.files_read.append(path)\n",
    "            print(\"File read successfully. Loading into vector store...\")\n",
    "\n",
    "        chunks: list[Document] = get_file_chunks(bytes, path)\n",
    "        return chunks\n",
    "    \n",
    "    def insert_data_to_chroma(self, chunks: list[Document]):\n",
    "        documents = []\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            data={\n",
    "                    \"embedding\": embedding(chunk.page_content),\n",
    "                    \"content\": chunk.page_content,\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"id\": index\n",
    "                }\n",
    "            documents.append(data)\n",
    "\n",
    "        embeddings = [data[\"embedding\"] for data in documents]\n",
    "        contents = [data[\"content\"] for data in documents]\n",
    "        metadatas = [data[\"metadata\"] for data in documents]\n",
    "        ids = [str(data[\"id\"]) for data in documents]\n",
    "\n",
    "        data={\n",
    "            \"embeddings\":embeddings,\n",
    "            \"contents\":contents,\n",
    "            \"metadatas\":metadatas,\n",
    "            \"ids\":ids\n",
    "        }\n",
    "        try:\n",
    "            self.chroma.add_data_to(\n",
    "                \"test\",\n",
    "                data=data\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509680a7-7bfa-4c36-8f2c-de561382bdfe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7198c87-61a0-477b-8d97-1130a8d24cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    \"\"\"\n",
    "        LLM Base class - Use this class to instance multiple-llms.\n",
    "        [See HF repository to view all llm models]\n",
    "    \"\"\"\n",
    "    # text generation\n",
    "    def __init__(self, default_model: str, tasks : list[str], initial_config: dict):\n",
    "        self.tasks = tasks\n",
    "        self.config = initial_config\n",
    "        self.default_model = default_model\n",
    "        try:\n",
    "            self.actions = {}\n",
    "            for pipe in self.task_create_pipelines(model=default_model, initial_config=initial_config):\n",
    "                self.actions[pipe.task] = pipe\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception(\"Error creating pipelines. Logs in console.\")\n",
    "    \n",
    "    def task_create_pipelines(self, model: str, initial_config: dict):\n",
    "        for task in self.tasks:\n",
    "            yield pipeline(task, model=model, trust_remote_code=True, model_kwargs=initial_config)\n",
    "\n",
    "    def task(self, task: str, prompt: str, **kwargs):\n",
    "        if task in self.tasks:\n",
    "            return self.actions[task](prompt, **kwargs)\n",
    "        return \"Task not available.\"\n",
    "    \n",
    "    def task_text(self, context:str, question:str):\n",
    "        generation = self.actions[\"question-answering\"](\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "        return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0eb35c-5fb0-422a-a722-3dc0296b3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_loader import Loader\n",
    "#from chroma import Chroma\n",
    "#from embeddings import embedding\n",
    "#from llms import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b149c0-5348-4cf9-bf37-9264f4eb34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully. Loading into vector store...\n",
      "ERROR TRYING TO GET CHUNKS FROM FILE\n"
     ]
    }
   ],
   "source": [
    "#ChromaInstance = Chroma()\n",
    "LoaderInstance = Loader(chroma_instance=ChromaInstance)\n",
    "\n",
    "# change it with your text file or pdf file path.\n",
    "example_path = \"LocalDocs/MIL-STD-882E.pdf\"\n",
    "\n",
    "LoaderInstance.load_document(example_path)\n",
    "\n",
    "# Use in True whether you wan't that the model uses less vram but you need a GPU.\n",
    "# Use in False whether you want to use it fully fp precision (will use more vram)\n",
    "quantized = True\n",
    "\n",
    "llm = LLM(default_model=\"timpal0l/mdeberta-v3-base-squad2\", tasks=[\"question-answering\"], initial_config={\n",
    "    \"device_map\":\"auto\"\n",
    "})\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"- Prompt: \")\n",
    "    context = ChromaInstance.query(embedding(user_query), {})\n",
    "    \n",
    "    user_query_template = create_template(context, user_query)\n",
    "  \n",
    "    completion = llm.task(\"question-answering\", user_query_template)\n",
    "\n",
    "    print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809df7b-e3b4-4632-95bc-3955d118c1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
