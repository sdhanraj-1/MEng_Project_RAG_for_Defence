{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "284e8e76-1629-4936-9e5e-2fda367a2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import os\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441aa95-fc47-44b8-bc45-7c1c4d49f03d",
   "metadata": {},
   "source": [
    "https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a378ce4f-0a1a-4d24-9241-9c6db0aa8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF files from a folder\n",
    "def load_pdfs(folder_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "            documents.extend(loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c157ffcc-dfc7-4259-ab58-9d5448eb783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=5000, chunk_overlap=50\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0041c080-9c04-403a-9d0b-d567268f3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma vector database\n",
    "def setup_chroma(docs, persist_directory=\"./chroma_db\"):\n",
    "    embeddings = OllamaEmbeddings(model=\"all-minilm\")  # Using model for embeddings\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a1ccb39-c2b2-4765-ae58-162abb51e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrieval and QA system\n",
    "def get_qa_chain(db):\n",
    "    retriever = db.as_retriever()\n",
    "    llm = OllamaLLM(model=\"llama3.2:1b\", num_gpu = -1)\n",
    "\n",
    "\n",
    "    system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78fa7edb-bcbd-4a13-a976-14e5d6f50078",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./LocalDocs\"  # Change this to your actual folder\n",
    "documents = load_pdfs(folder_path)\n",
    "split_docs = split_documents(documents)\n",
    "db = setup_chroma(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f57292e5-f47c-4bd7-9524-4bc3404400ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"all-minilm\")  # Using model for embeddings\n",
    "persist_directory = \"./chroma_db\"\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "qa_chain = get_qa_chain(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01fa940d-993d-4b50-94b5-87ce88af5440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='ba5e2340-d9b5-4b39-9371-a13b2690fdcf', metadata={'page': 101, 'page_label': '102', 'source': './LocalDocs\\\\MIL-STD-882E.pdf'}, page_content='high, serious, or medium safety risk. \\n\\uf0b7 A requirement that, if implemented, would negatively impact safety; however \\ncode is implemented safely. \\n \\ne.  Defining and following a process for assessing risk associated with hazards is critical \\nto the success of a program, particularly as systems are combined into more complex SoS.  These \\nSoS often involve systems developed under disparate development and safety programs and may')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.invoke(\"safety\")\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e5047f-dc25-417f-b32d-997665b1283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: {'query': 'define software safety', 'result': 'According to the provided context, Software Safety refers to the practice of ensuring that software systems are designed, developed, tested, and used in a way that minimizes the likelihood of failures, hazards, and mishaps. It involves identifying and mitigating the exact software contributors to hazards, as well as increasing confidence that the software will perform as specified to safety and performance requirements while reducing the number of contributors to hazards.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"define software safety\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502e7002-0247-4b2a-84fd-4956f0af1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121b86e-974d-4e0f-ba8d-917a4cac402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | f()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84ba04-120b-4bb3-8250-8be0427ecd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cd591-0664-4f77-9f63-b9c535cb7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74be86-1bde-47fb-930d-52bbcdae1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e025c-f0b2-4de1-a667-d9e0de7fb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fd958-943a-4551-a4bb-c3798212f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
