{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284e8e76-1629-4936-9e5e-2fda367a2c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5be7b-e712-41f7-bcaa-7e737580c616",
   "metadata": {},
   "source": [
    "https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57292e5-f47c-4bd7-9524-4bc3404400ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f90d1b-6644-4d04-bc80-63928691670d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"LocalDocs/MIL-STD-882E.pdf\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b673c70a-35cf-4e63-a011-f9b82b880d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf31aea-6c44-4230-819a-eaf9c987aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c510951-fafc-422f-a97d-140f1b49e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are system safety requirements?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8266b58e-e302-44e8-9eca-2a7b963b8ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='75faf8dd-19cd-49fb-8aa8-df12ec268c0b', metadata={'page': 14, 'page_label': '15', 'source': 'LocalDocs/MIL-STD-882E.pdf'}, page_content='4.2  System safety requirements.  Section 4 defines the system safety requirements \\nthroughout the life-cycle for any system.  When properly applied, these requirements should \\nenable the identification and management of hazards and their associated risks during system \\ndevelopmental and sustaining engineering activities. It is not the intent of this document to make \\nsystem safety personnel responsible for hazard management in other functional disciplines.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f487f4-b5cf-4380-8fdc-cf9990a8d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    num_gpu = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf909c8a-1554-4b04-b327-4a2516da4720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Scene:** A dark, crowded nightclub. The air is electric with anticipation as two of the most formidable voices in comedy take to the mic. In one corner, we have Stephen Colbert, the former host of \"The Daily Show\" and current host of \"The Late Show.\" Across from him, John Oliver, the charismatic and irreverent creator of \"Last Week Tonight\" stands tall, ready to take on the king of satire.\n",
      "\n",
      "**Stephen Colbert:**\n",
      "Yo, J-O-N, I hear you've been talking smack\n",
      "About my humor, about my comedic pack\n",
      "But let me tell you, I'm the one who's got the skill\n",
      "I can make a joke that'll make your head spin and thrill\n",
      "\n",
      "My wit's sharp as a razor, my sarcasm's on point\n",
      "I can skewer politicians like a hot knife through a bone\n",
      "You may have your fancy writing, your clever rhymes\n",
      "But when it comes to real comedy, I'm the one who shines\n",
      "\n",
      "**John Oliver:**\n",
      "Hold up, Steve, let me interrupt your flow\n",
      "You think you're funny, but your humor's just a show\n",
      "I can make fun of you, and my audience will too\n",
      "You may have your loyal fans, but I've got a whole crew\n",
      "\n",
      "My satire's not just jokes, it's a cry for help\n",
      "A call to arms against the absurd, the ridiculous, the Yelp-ful\n",
      "I expose the hypocrisy, the corruption, the lies\n",
      "You may be able to laugh, but I'll make you realize\n",
      "\n",
      "The truth is ugly, Steve, and sometimes we must face\n",
      "The horrors of politics, the depths of our own place\n",
      "My comedy's not just funny, it's a warning sign\n",
      "A reminder that the world's a crazy, messed-up thing, all the time\n",
      "\n",
      "**Stephen Colbert:**\n",
      "Oh please, John, you may have your little crowd\n",
      "But when it comes to real comedy, I'm the one who's loud\n",
      "I'll make fun of the elites, the rich and the famous\n",
      "You can just sit there in your posh London house, it's all a joke\n",
      "\n",
      "My humor's not just about me, it's about you and yours\n",
      "The absurdity of life, the ridiculous, the impossible to endure\n",
      "I'm a master of irony, I can make fun of my own flaws\n",
      "But you're just a pompous, self-absorbed, know-it-all for all\n",
      "\n",
      "**John Oliver:**\n",
      "That's rich coming from a man who used to host a news show\n",
      "About politics and current events, the real world, not just a joke\n",
      "You think you're funny, but your humor's just a tired old trick\n",
      "I'm the one who'll make you laugh, while you're stuck on quicksand\n",
      "\n",
      "My comedy's about something bigger than yourself, Steve\n",
      "It's about the human condition, with all its flaws and relief\n",
      "I may not be as flashy, but I'm honest and true\n",
      "And when it comes to real comedy, I'm the one who'll see you through\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e7002-0247-4b2a-84fd-4956f0af1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121b86e-974d-4e0f-ba8d-917a4cac402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84ba04-120b-4bb3-8250-8be0427ecd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cd591-0664-4f77-9f63-b9c535cb7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74be86-1bde-47fb-930d-52bbcdae1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e025c-f0b2-4de1-a667-d9e0de7fb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fd958-943a-4551-a4bb-c3798212f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
